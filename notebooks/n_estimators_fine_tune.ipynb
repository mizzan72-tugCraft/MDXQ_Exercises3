# ã‚¹ã‚³ã‚¢è¨˜éŒ²ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
from score_analysis import record_score

print("\n" + "=" * 70)
print("ğŸ“ ã‚¹ã‚³ã‚¢è¨˜éŒ²ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæå‡ºå¾Œã«å®Ÿè¡Œï¼‰")
print("=" * 70)
print("æå‡ºå¾Œã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„:\n")

for n_est in n_estimators_fine:
    print(f"# n_estimators={n_est}ã®ã‚¹ã‚³ã‚¢è¨˜éŒ²")
    print(f"score_{n_est} = None  # æå‡ºå¾Œã«ã‚¹ã‚³ã‚¢ã‚’å…¥åŠ›")
    print(f"if score_{n_est} is not None:")
    print(f"    record_score(")
    print(f"        current_score=score_{n_est},")
    print(f"        model_name='RandomForestRegressor (n_estimators={n_est})',")
    print(f"        features_used='åº—èˆ—ID, å•†å“ID, å¹´, æœˆ, å•†å“ã‚«ãƒ†ã‚´ãƒªID',")
    print(f"        notes='ç´°å¯†æœ€é©åŒ–å®Ÿé¨“'")
    print(f"    )")
    print()

print("=" * 70)
print("ğŸ¯ æœ€é©å€¤ç™ºè¦‹å¾Œã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print("1. æœ€è‰¯ã®n_estimatorså€¤ã‚’ç‰¹å®š")
print("2. ãã®è¨­å®šã§ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½")
print("3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´ãªã‚‹èª¿æ•´")
print("4. ä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆXGBoost, LightGBMï¼‰ã¨ã®æ¯”è¼ƒ")
print("=" * 70)## ğŸ¯ 100ä»¥ä¸‹ã§ã®æœ€é©å€¤äºˆæ¸¬ã¨ç†è«–çš„è€ƒå¯Ÿ

### äºˆæ¸¬ã•ã‚Œã‚‹çµæœãƒ‘ã‚¿ãƒ¼ãƒ³

| n_estimators | äºˆæ¸¬ã•ã‚Œã‚‹æ€§èƒ½ | æ ¹æ‹  |
|-------------|-------------|------|
| **10** | å¤§å¹…ã«æ‚ªåŒ– | ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœä¸è¶³ã€é«˜ãƒã‚¤ã‚¢ã‚¹ |
| **25** | ã‚„ã‚„æ‚ªåŒ– | ä¸å®‰å®šæ€§ã‚ã‚Šã€ã¾ã æœ¨ãŒå°‘ãªã„ |
| **50** | **è‰¯å¥½ãªå€™è£œ** | ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ã€åŠ¹ç‡çš„ |
| **75** | **æœ€å„ªç§€å€™è£œ** | 100ã«è¿‘ã„æ€§èƒ½ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ |
| **100** | æ—¢çŸ¥ã®ãƒ™ã‚¹ãƒˆ | ç¾åœ¨ã®æœ€è‰¯ã‚¹ã‚³ã‚¢ |

### æœ€é©å€¤ã®ç†è«–çš„æ¨å®š

**ä»®èª¬**: **n_estimators=75ãŒæœ€é©å€¤**ã®å¯èƒ½æ€§ãŒé«˜ã„

**ç†ç”±**:
1. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœã®é£½å’Œç‚¹**: ä¸€èˆ¬çš„ã«50-100ã§æ€§èƒ½ãŒé£½å’Œ
2. **éå­¦ç¿’å›é¿**: 200ã§æ‚ªåŒ–ã—ãŸãŸã‚ã€100ä»¥ä¸‹ã«æœ€é©å€¤
3. **è¨ˆç®—åŠ¹ç‡**: 75ãªã‚‰25%ã®è¨ˆç®—æ™‚é–“çŸ­ç¸®
4. **å®‰å®šæ€§**: 75æœ¬ã‚ã‚Œã°ååˆ†ãªäºˆæ¸¬å®‰å®šæ€§

### æœŸå¾…ã•ã‚Œã‚‹å®Ÿé¨“çµæœ

```
äºˆæ¸¬ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°:
1. n_estimators=75  â† æœ€å„ªç§€å€™è£œ
2. n_estimators=100 â† æ—¢çŸ¥ã®ãƒ™ã‚¹ãƒˆ
3. n_estimators=50  â† åŠ¹ç‡é‡è¦–å€™è£œ
4. n_estimators=25  â† ã‚„ã‚„åŠ£ã‚‹
5. n_estimators=10  â† å¤§å¹…ã«åŠ£ã‚‹
```

### ãƒ“ã‚¸ãƒã‚¹çš„ä¾¡å€¤

- **75ãŒæœ€é©ãªã‚‰**: 25%ã®è¨ˆç®—æ™‚é–“çŸ­ç¸® + åŒç­‰æ€§èƒ½
- **50ãŒååˆ†ãªã‚‰**: 50%ã®è¨ˆç®—æ™‚é–“çŸ­ç¸® + è¨±å®¹å¯èƒ½ãªæ€§èƒ½ä½ä¸‹
- **å®Ÿç”¨æ€§**: ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–ã€ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡åŒ–# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
now = datetime.datetime.now()
timestamp = now.strftime("%Y%m%d_%H%M%S")

print("\n" + "=" * 60)
print("ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ")
print("=" * 60)

submission_files_fine = {}
for n_est in n_estimators_fine:
    submission_copy = submission_df.copy()
    submission_copy[1] = results_fine[n_est]['predictions']
    
    filename = f'../submissions/{timestamp}_Exercises3_Challenge_rf{n_est}_fine.csv'
    submission_copy.to_csv(filename, index=False, header=False)
    
    submission_files_fine[n_est] = filename
    print(f"âœ… n_estimators={n_est:2d}: {filename}")

print(f"\nğŸ¯ {len(submission_files_fine)}å€‹ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
print("å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ãƒˆã«æå‡ºã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨å¯è¦–åŒ–
plt.figure(figsize=(15, 10))

# å­¦ç¿’æ™‚é–“ã®å¯è¦–åŒ–
plt.subplot(2, 3, 1)
times = [results_fine[n]['training_time'] for n in n_estimators_fine]
plt.plot(n_estimators_fine, times, 'bo-', linewidth=2, markersize=8)
plt.xlabel('n_estimators')
plt.ylabel('å­¦ç¿’æ™‚é–“ (ç§’)')
plt.title('å­¦ç¿’æ™‚é–“ vs n_estimators')
plt.grid(True, alpha=0.3)

# äºˆæ¸¬å€¤ã®å¹³å‡
plt.subplot(2, 3, 2)
means = [results_fine[n]['pred_mean'] for n in n_estimators_fine]
plt.plot(n_estimators_fine, means, 'go-', linewidth=2, markersize=8)
plt.xlabel('n_estimators')
plt.ylabel('äºˆæ¸¬å€¤ã®å¹³å‡')
plt.title('äºˆæ¸¬å€¤ã®å¹³å‡ vs n_estimators')
plt.grid(True, alpha=0.3)

# äºˆæ¸¬å€¤ã®æ¨™æº–åå·®
plt.subplot(2, 3, 3)
stds = [results_fine[n]['pred_std'] for n in n_estimators_fine]
plt.plot(n_estimators_fine, stds, 'ro-', linewidth=2, markersize=8)
plt.xlabel('n_estimators')
plt.ylabel('äºˆæ¸¬å€¤ã®æ¨™æº–åå·®')
plt.title('äºˆæ¸¬å€¤ã®ãƒãƒ©ãƒ„ã‚­ vs n_estimators')
plt.grid(True, alpha=0.3)

# äºˆæ¸¬å€¤ã®åˆ†å¸ƒæ¯”è¼ƒï¼ˆç®±ã²ã’å›³ï¼‰
plt.subplot(2, 3, 4)
pred_data = [results_fine[n]['predictions'] for n in n_estimators_fine]
plt.boxplot(pred_data, labels=n_estimators_fine)
plt.xlabel('n_estimators')
plt.ylabel('äºˆæ¸¬å€¤')
plt.title('äºˆæ¸¬å€¤ã®åˆ†å¸ƒæ¯”è¼ƒ')
plt.yscale('log')  # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«

# åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆå­¦ç¿’æ™‚é–“ã®é€†æ•°ï¼‰
plt.subplot(2, 3, 5)
efficiency = [1/results_fine[n]['training_time'] for n in n_estimators_fine]
plt.plot(n_estimators_fine, efficiency, 'mo-', linewidth=2, markersize=8)
plt.xlabel('n_estimators')
plt.ylabel('åŠ¹ç‡æ€§ (1/å­¦ç¿’æ™‚é–“)')
plt.title('è¨ˆç®—åŠ¹ç‡æ€§ vs n_estimators')
plt.grid(True, alpha=0.3)

# äºˆæ¸¬å€¤ã®ç¯„å›²
plt.subplot(2, 3, 6)
ranges = [results_fine[n]['pred_max'] - results_fine[n]['pred_min'] for n in n_estimators_fine]
plt.plot(n_estimators_fine, ranges, 'co-', linewidth=2, markersize=8)
plt.xlabel('n_estimators')
plt.ylabel('äºˆæ¸¬å€¤ã®ç¯„å›²')
plt.title('äºˆæ¸¬å€¤ã®ç¯„å›² vs n_estimators')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# è©³ç´°ãªæ•°å€¤æ¯”è¼ƒè¡¨
print("\n" + "=" * 80)
print("ğŸ“Š è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒè¡¨")
print("=" * 80)
print(f"{'n_est':>6} {'å­¦ç¿’æ™‚é–“':>8} {'äºˆæ¸¬å¹³å‡':>8} {'äºˆæ¸¬std':>8} {'äºˆæ¸¬ç¯„å›²':>8} {'åŠ¹ç‡æ€§':>8}")
print("-" * 80)
for n_est in n_estimators_fine:
    r = results_fine[n_est]
    efficiency = 1/r['training_time']
    pred_range = r['pred_max'] - r['pred_min']
    print(f"{n_est:>6} {r['training_time']:>8.2f} {r['pred_mean']:>8.2f} {r['pred_std']:>8.2f} {pred_range:>8.2f} {efficiency:>8.2f}")
print("=" * 80)# ç´°å¯†æœ€é©åŒ–å®Ÿé¨“ï¼ˆ100ä»¥ä¸‹ï¼‰
n_estimators_fine = [10, 25, 50, 75, 100]
results_fine = {}
training_times = {}

print("=" * 70)
print("ğŸ” n_estimatorsç´°å¯†æœ€é©åŒ–å®Ÿé¨“ï¼ˆ100ä»¥ä¸‹ï¼‰")
print("=" * 70)
print("ç›®æ¨™: è¨ˆç®—åŠ¹ç‡ã¨äºˆæ¸¬ç²¾åº¦ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã‚’ç™ºè¦‹")
print("=" * 70)

for n_est in n_estimators_fine:
    print(f"\nğŸŒ³ n_estimators={n_est} ã§å®Ÿé¨“ä¸­...")
    
    # å­¦ç¿’æ™‚é–“ã®æ¸¬å®šé–‹å§‹
    start_time = time.time()
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’
    model = RandomForestRegressor(
        n_estimators=n_est, 
        random_state=42, 
        n_jobs=-1,
        max_features='sqrt'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’æ˜ç¤º
    )
    model.fit(X_train, y_train)
    
    # å­¦ç¿’æ™‚é–“ã®æ¸¬å®šçµ‚äº†
    training_time = time.time() - start_time
    
    # äºˆæ¸¬
    y_pred = model.predict(X_test)
    y_pred[y_pred < 0] = 0
    
    # çµæœã‚’ä¿å­˜
    results_fine[n_est] = {
        'model': model,
        'predictions': y_pred,
        'pred_min': y_pred.min(),
        'pred_max': y_pred.max(),
        'pred_mean': y_pred.mean(),
        'pred_std': y_pred.std(),
        'training_time': training_time
    }
    
    print(f"   â±ï¸  å­¦ç¿’æ™‚é–“: {training_time:.2f}ç§’")
    print(f"   ğŸ“Š äºˆæ¸¬å€¤ã®ç¯„å›²: {y_pred.min():.2f} ï½ {y_pred.max():.2f}")
    print(f"   ğŸ“ˆ äºˆæ¸¬å€¤ã®å¹³å‡: {y_pred.mean():.2f}")
    print(f"   ğŸ“ äºˆæ¸¬å€¤ã®æ¨™æº–åå·®: {y_pred.std():.2f}")

print("\nâœ… å…¨ã¦ã®ç´°å¯†å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸ")# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼ˆãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã¨åŒä¸€ï¼‰
base_dir = '../data/'
sales_df = pd.read_csv(base_dir + 'sales_history.csv')
item_categories_df = pd.read_csv(base_dir + 'item_categories.csv')
category_names_df = pd.read_csv(base_dir + 'category_names.csv')
test_df = pd.read_csv(base_dir + 'test.csv')
submission_df = pd.read_csv(base_dir + 'sample_submission.csv', header=None)

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
sales_df['å¹´'] = sales_df['æ—¥ä»˜'].apply(lambda x: x.split('-')[0])
sales_df['æœˆ'] = sales_df['æ—¥ä»˜'].apply(lambda x: x.split('-')[1])
sales_month_df = sales_df.groupby(['å•†å“ID', 'åº—èˆ—ID', 'å¹´', 'æœˆ'])['å£²ä¸Šå€‹æ•°'].sum().reset_index()
train_df = pd.merge(sales_month_df, item_categories_df, on='å•†å“ID', how='left')
train_df['å¹´'] = train_df['å¹´'].astype(int)
train_df['æœˆ'] = train_df['æœˆ'].astype(int)

test_df['å¹´'] = 2022
test_df['æœˆ'] = 12
test_df = pd.merge(test_df, item_categories_df, on='å•†å“ID', how='left')

feature_columns = ['åº—èˆ—ID', 'å•†å“ID', 'å¹´', 'æœˆ', 'å•†å“ã‚«ãƒ†ã‚´ãƒªID']
target_column = 'å£²ä¸Šå€‹æ•°'
X_train = train_df[feature_columns]
y_train = train_df[target_column]
X_test = test_df[feature_columns]

print("âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_train.shape}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_test.shape}")# =============================================================================
# n_estimatorsç´°å¯†æœ€é©åŒ–å®Ÿé¨“ï¼ˆ100ä»¥ä¸‹ï¼‰
# =============================================================================
# 
# ã€å®Ÿé¨“ç›®çš„ã€‘
# - n_estimators=100ãŒãƒ™ã‚¹ãƒˆï¼ˆ3.07256739424164ï¼‰
# - n_estimators=200ã§æ‚ªåŒ–ï¼ˆ3.078811806610283ï¼‰
# - 100ä»¥ä¸‹ã§ã®æœ€é©å€¤ã‚’ç´°ã‹ãæ¢ç´¢
# 
# ã€å®Ÿé¨“ç¯„å›²ã€‘
# [10, 25, 50, 75, 100] - ã‚ˆã‚Šç´°ã‹ã„åˆ»ã¿ã§æœ€é©å€¤ã‚’ç‰¹å®š
# 
# ã€æœŸå¾…ã•ã‚Œã‚‹çµæœã€‘
# - 10: ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆé«˜ãƒã‚¤ã‚¢ã‚¹ï¼‰
# - 25-75: æœ€é©å€¤å€™è£œç¯„å›²
# - 100: æ—¢çŸ¥ã®ãƒ™ã‚¹ãƒˆå€¤
# 
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import datetime
import os
import time