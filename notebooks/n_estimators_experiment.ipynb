# ã‚¹ã‚³ã‚¢è¨˜éŒ²ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæå‡ºå¾Œã«å®Ÿè¡Œï¼‰
from score_analysis import record_score

print("\nğŸ“ ã‚¹ã‚³ã‚¢è¨˜éŒ²ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
print("æå‡ºå¾Œã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„:\n")

for n_est in n_estimators_values:
    print(f"# n_estimators={n_est}ã®ã‚¹ã‚³ã‚¢è¨˜éŒ²")
    print(f"score_{n_est} = None  # æå‡ºå¾Œã«ã‚¹ã‚³ã‚¢ã‚’å…¥åŠ›")
    print(f"if score_{n_est} is not None:")
    print(f"    record_score(")
    print(f"        current_score=score_{n_est},")
    print(f"        model_name='RandomForestRegressor (n_estimators={n_est})',")
    print(f"        features_used='åº—èˆ—ID, å•†å“ID, å¹´, æœˆ, å•†å“ã‚«ãƒ†ã‚´ãƒªID',")
    print(f"        notes='n_estimatorsæœ€é©åŒ–å®Ÿé¨“'")
    print(f"    )")
    print()

print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: æœ€ã‚‚è‰¯ã„ã‚¹ã‚³ã‚¢ãŒå‡ºãŸè¨­å®šã§è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¾ã—ã‚‡ã†ï¼")## ğŸ§  n_estimatorsæœ€é©åŒ–ã®ç†è«–çš„è€ƒå¯Ÿ

### n_estimatorsã¨RandomForestã®é–¢ä¿‚

**1. ãƒã‚¤ã‚¢ã‚¹-ãƒãƒªã‚¢ãƒ³ã‚¹ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**
- **å°‘ãªã„æœ¨**: é«˜ãƒã‚¤ã‚¢ã‚¹ã€ä½ãƒãƒªã‚¢ãƒ³ã‚¹ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼‰
- **å¤šã„æœ¨**: ä½ãƒã‚¤ã‚¢ã‚¹ã€é«˜ãƒãƒªã‚¢ãƒ³ã‚¹ï¼ˆéå­¦ç¿’ã®å¯èƒ½æ€§ï¼‰

**2. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ:**
- è¤‡æ•°ã®æ±ºå®šæœ¨ã®äºˆæ¸¬ã‚’å¹³å‡åŒ–
- å€‹ã€…ã®æœ¨ã®äºˆæ¸¬èª¤å·®ã‚’ç›¸æ®º
- ä¸€èˆ¬çš„ã«æœ¨ã®æ•°ãŒå¢—ãˆã‚‹ã»ã©å®‰å®š

**3. è¨ˆç®—ã‚³ã‚¹ãƒˆã¨æ€§èƒ½:**
- å­¦ç¿’æ™‚é–“: O(n_estimators Ã— ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º)
- äºˆæ¸¬æ™‚é–“: O(n_estimators Ã— æœ¨ã®æ·±ã•)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: O(n_estimators Ã— æœ¨ã®ã‚µã‚¤ã‚º)

### ç¾åœ¨ã®çµæœã‹ã‚‰ã®æ¨æ¸¬
- `n_estimators=100`ãŒãƒ™ã‚¹ãƒˆ â†’ ã“ã®å•é¡Œã«å¯¾ã™ã‚‹æœ€é©å€¤ä»˜è¿‘
- `n_estimators=200`ã§æ‚ªåŒ– â†’ éå­¦ç¿’ã¾ãŸã¯è¨ˆç®—ã®ä¸å®‰å®šæ€§
- ã‚ˆã‚Šå°ã•ã„å€¤ã§ã®å®Ÿé¨“ â†’ è¨ˆç®—åŠ¹ç‡ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ç¢ºèª

### æœŸå¾…ã•ã‚Œã‚‹çµæœ
- `n_estimators=50`: 100ã«è¿‘ã„æ€§èƒ½ã€ã‚ˆã‚Šé«˜é€Ÿ
- `n_estimators=25`: ã‚„ã‚„æ€§èƒ½ä½ä¸‹ã€å¤§å¹…ãªé«˜é€ŸåŒ–
- æœ€é©å€¤ã¯25-100ã®é–“ã«å­˜åœ¨ã™ã‚‹å¯èƒ½æ€§# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆå„n_estimatorså€¤ã«å¯¾ã—ã¦ï¼‰
now = datetime.datetime.now()
timestamp = now.strftime("%Y%m%d_%H%M%S")

submission_files = {}

for n_est in n_estimators_values:
    # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    submission_copy = submission_df.copy()
    submission_copy[1] = results[n_est]['predictions']
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    filename = f'../submissions/{timestamp}_Exercises3_Challenge_rf{n_est}.csv'
    submission_copy.to_csv(filename, index=False, header=False)
    
    submission_files[n_est] = filename
    print(f"âœ… n_estimators={n_est}: {filename}")

print(f"\nğŸ¯ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼ˆ{len(submission_files)}å€‹ï¼‰")
print("å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ãƒˆã«æå‡ºã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")# n_estimatorså®Ÿé¨“
n_estimators_values = [25, 50, 100, 200]
results = {}
predictions = {}

print("=" * 60)
print("ğŸ”¬ n_estimatorsæœ€é©åŒ–å®Ÿé¨“")
print("=" * 60)

for n_est in n_estimators_values:
    print(f"\nğŸŒ³ n_estimators={n_est} ã§å­¦ç¿’ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã¨å­¦ç¿’
    model = RandomForestRegressor(n_estimators=n_est, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    
    # äºˆæ¸¬
    y_pred = model.predict(X_test)
    y_pred[y_pred < 0] = 0  # è² ã®å€¤ã‚’0ã«å¤‰æ›
    
    # çµæœã‚’ä¿å­˜
    results[n_est] = {
        'model': model,
        'predictions': y_pred,
        'pred_min': y_pred.min(),
        'pred_max': y_pred.max(),
        'pred_mean': y_pred.mean()
    }
    
    print(f"   äºˆæ¸¬å€¤ã®ç¯„å›²: {y_pred.min():.2f} ï½ {y_pred.max():.2f}")
    print(f"   äºˆæ¸¬å€¤ã®å¹³å‡: {y_pred.mean():.2f}")

print("\nâœ… å…¨ã¦ã®å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸ")# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã¨åŒã˜å‡¦ç†ï¼‰
sales_df['å¹´'] = sales_df['æ—¥ä»˜'].apply(lambda x: x.split('-')[0])
sales_df['æœˆ'] = sales_df['æ—¥ä»˜'].apply(lambda x: x.split('-')[1])

# æœˆã”ã¨ã®å£²ä¸Šå€‹æ•°ã‚’é›†è¨ˆ
sales_month_df = sales_df.groupby(['å•†å“ID', 'åº—èˆ—ID', 'å¹´', 'æœˆ'])['å£²ä¸Šå€‹æ•°'].sum().reset_index()

# å•†å“ã‚«ãƒ†ã‚´ãƒªIDã‚’çµåˆ
train_df = pd.merge(sales_month_df, item_categories_df, on='å•†å“ID', how='left')

# ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
train_df['å¹´'] = train_df['å¹´'].astype(int)
train_df['æœˆ'] = train_df['æœˆ'].astype(int)

# test_dfã«ã‚‚å¹´ã¨æœˆã‚’è¿½åŠ ã—ã€å•†å“ã‚«ãƒ†ã‚´ãƒªIDã‚’çµåˆ
test_df['å¹´'] = 2022
test_df['æœˆ'] = 12
test_df = pd.merge(test_df, item_categories_df, on='å•†å“ID', how='left')

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å®šç¾©
feature_columns = ['åº—èˆ—ID', 'å•†å“ID', 'å¹´', 'æœˆ', 'å•†å“ã‚«ãƒ†ã‚´ãƒªID']
target_column = 'å£²ä¸Šå€‹æ•°'

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
X_train = train_df[feature_columns]
y_train = train_df[target_column]
X_test = test_df[feature_columns]

print("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_train.shape}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_test.shape}")# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
base_dir = '../data/'
sales_df = pd.read_csv(base_dir + 'sales_history.csv')
item_categories_df = pd.read_csv(base_dir + 'item_categories.csv')
category_names_df = pd.read_csv(base_dir + 'category_names.csv')
test_df = pd.read_csv(base_dir + 'test.csv')
submission_df = pd.read_csv(base_dir + 'sample_submission.csv', header=None)# =============================================================================
# n_estimatorsæœ€é©åŒ–å®Ÿé¨“
# =============================================================================
# 
# ã€å®Ÿé¨“ç›®çš„ã€‘
# - n_estimators=100ãŒãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ï¼ˆ3.07256739424164ï¼‰
# - n_estimators=200ã§æ‚ªåŒ–ï¼ˆ3.078811806610283ï¼‰
# - ã‚ˆã‚Šå°ã•ã„å€¤ï¼ˆ50, 25ï¼‰ã§ã®æ€§èƒ½ã‚’æ¤œè¨¼
# 
# ã€n_estimatorsã¨ã¯ã€‘
# - RandomForestã§ä½¿ç”¨ã™ã‚‹æ±ºå®šæœ¨ã®å€‹æ•°
# - å¤šã™ãã‚‹â†’éå­¦ç¿’ã€è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
# - å°‘ãªã™ãã‚‹â†’ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€ä¸å®‰å®š
# - æœ€é©å€¤ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒé‡è¦
# 
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import datetime
import os